---
title: "Measuring and Mitigating Disparity of Decision-Making Tools"
author: "Ben Brintz"
institute: "Division of Epidemiology"
format: 
  revealjs:
    auto-animate-easing: ease-in-out
    auto-animate-unmatched: false
    auto-animate-duration: 2
    revealjs-plugins: 
    theme: white
    incremental: true
    slideNumber: true  # Slide numbers (optional)
    transition: fade   # Smooth slide transitions
    title-slide-background: "white" # Ensure title slide background matches your style
    css: custom.css    # Link to your custom CSS file
---



## There is some controversy surrounding the eGFR equation {style="font-size:.75em;" auto-animate=true auto-animate-easing="ease-in-out"}


::: {.r-hstack}
  <img src="egfr1.gif" alt="GIF 1" style="height:160px; width:auto; object-fit:contain;">
  <img src="egfr2.gif" alt="GIF 2" style="height:160px; width:auto; object-fit:contain;">
  <img src="egfr3.gif" alt="GIF 3" style="height:160px; width:auto; object-fit:contain;">
:::

## There is some controversy surrounding the eGFR equation {style="font-size:.75em;" auto-animate=true auto-animate-easing="ease-in-out" visibility="uncounted"}
::: {.r-hstack}
  <img src="egfr1.gif" alt="GIF 1" style="height:160px; width:auto; object-fit:contain;">
  <img src="egfr2.gif" alt="GIF 2" style="height:160px; width:auto; object-fit:contain;">
  <img src="egfr3.gif" alt="GIF 3" style="height:160px; width:auto; object-fit:contain;">
:::
::: {.r-hstack}
  <img src="egfr4.gif" alt="GIF 1" style="height:160px; width:auto; object-fit:contain;">
  <img src="egfr5.gif" alt="GIF 2" style="height:160px; width:auto; object-fit:contain;">
  <img src="egfr6.gif" alt="GIF 3" style="height:160px; width:auto; object-fit:contain;">
:::

## There is some controversy surrounding the eGFR equation {style="font-size:.75em;" auto-animate=true auto-animate-easing="ease-in-out" visibility="uncounted"}
::: {.r-hstack}
  <img src="egfr1.gif" alt="GIF 1" style="height:160px; width:auto; object-fit:contain;">
  <img src="egfr2.gif" alt="GIF 2" style="height:160px; width:auto; object-fit:contain;">
  <img src="egfr3.gif" alt="GIF 3" style="height:160px; width:auto; object-fit:contain;">
:::
::: {.r-hstack}
  <img src="egfr4.gif" alt="GIF 1" style="height:160px; width:auto; object-fit:contain;">
  <img src="egfr5.gif" alt="GIF 2" style="height:160px; width:auto; object-fit:contain;">
  <img src="egfr6.gif" alt="GIF 3" style="height:160px; width:auto; object-fit:contain;">
:::
::: {.r-hstack}
  <img src="egfr7.gif" alt="GIF 1" style="height:160px; width:auto; object-fit:contain;">
  <img src="egfr8.gif" alt="GIF 2" style="height:160px; width:auto; object-fit:contain;">
  <img src="egfr9.gif" alt="GIF 3" style="height:160px; width:auto; object-fit:contain;">
:::

## The NKF and ASN have since recommended removal of race from the equation


- Acknowledged race is a social concept 
    - i.e., it's a system to classify individuals rather than reflect biology
    - I have heard the biology is more regional than racial 
-  Does removal of race reduce performance of the decision-making tool?



::: {.fragment .fade-up}
 It depends on how you're measuring performance
:::

---
<div style="font-size: 60px; text-align: center;">Performance metrics are a trade-off </div>
```{r, message=F}
library(tidyverse)
library(pROC)
out=aSAH %>% filter(gender=="Female") %>% roc(outcome,s100b,plot=F,smooth=T)
data.frame(x=out$specificities,y=out$sensitivities) %>%
ggplot(aes(x=1-x,y=y)) + geom_line() + geom_abline(color="grey") + theme_bw() +
ylab("Sensitivity") + xlab("Specificity") + theme(aspect.ratio=.75) +
scale_x_continuous(breaks=c(0,.25,.5,.75,1),labels=rev(c(0,.25,.5,.75,1))) + 
ggtitle("ROC Curve")
```
::: {.fragment .fade-in-then-semi-out}
The developer of a tool can choose to emphasize one metric over another 
:::

::: {.fragment .fade-right}
And the choice of metric could be predictive performance or fairness 
:::


## Some fairness metrics are more well known than others {.smaller .scrollable}
<style>
.reveal .fragment {
  margin-bottom: 5px; /* Adjust this value to reduce space between fragments */
}
</style>


::: {.fragment  fragment-index=1}
\begin{align*}
\text{Statistical Parity} &= P(\widehat{Y}=1|A=a) \\
&= P(\widehat{Y}=1|A=b)
\end{align*}
:::

::: {.fragment fragment-index=2}
\begin{align*}
\text{Equalized Odds} &= P(\widehat{Y}=1|A=a,Y=1) \\
&= P(\widehat{Y}=1|A=b,Y=1)
\end{align*}
:::

::: {.fragment fragment-index=3}
\begin{align*}
\text{Predictive Parity} &= P(Y=1|\widehat{Y}=1,A=a) \\
&= P(Y=1|\widehat{Y}=1,A=b)
\end{align*}
:::

::: {.fragment fragment-index=4}
\begin{align*}
\text{Balance for the Positive Class} &= E(S|Y=1,A=a) \\ &=E(S|Y=1,A=b)
\end{align*}
:::

## The COMPAS data is a landmark dataset to study algorithmic fairness in recidivism prediction (propietary algorithm) {auto-animate="true" .scrollable}
```r
library(fairness)

head(compas)
```

## The COMPAS data is a landmark dataset to study algorithmic fairness in recidivism prediction (propietary algorithm) {auto-animate="true" visibility="uncounted" .scrollable}
```{r, include=T}
library(fairness)
library(tidyverse)
library(knitr)
library(kableExtra)
head(compas) #%>% knitr::kable()# %>% kableExtra::kable_styling(full_width = F) #%>% 
  #kableExtra::row_spec(1:6,font_size='medium')
```

```{r, include=F}
library(fairness)
library(tidyverse)
library(knitr)
library(kableExtra)
c2=compas #%>% knitr::kable()# %>% kableExtra::kable_styling(full_width = F) #%>% 
  #kableExtra::row_spec(1:6,font_size='medium')
```

## If you can code, measuring fairness is easy {auto-animate="true" .smaller .scrollable }
```r
a=compas %>% group_by(Female) %>% summarize(`Statistical Parity`=mean(predicted))

b=compas %>% filter(Two_yr_Recidivism=="yes") %>% group_by(Female) %>% summarize(`Equalized Odds`=mean(predicted))

c=compas %>% filter(predicted==1) %>% group_by(Female) %>% summarize('Predictive Parity'=mean(Two_yr_Recidivism=="yes"))

d=compas %>% filter(Two_yr_Recidivism=="yes") %>% group_by(Female) %>% summarize('Balance for the Positive Class'=mean(probability))
```

## If you can code, measuring fairness is easy {auto-animate="true" .smaller .scrollable visibility="uncounted"}
```{r,include=T,echo=T}
a=compas %>% group_by(Sex=Female) %>% summarize(`Statistical Parity`=mean(predicted))

b=compas %>% filter(Two_yr_Recidivism=="yes") %>% group_by(Female) %>% summarize(`Equalized Odds`=mean(predicted)) %>% select(-Female)

c=compas %>% filter(predicted==1) %>% group_by(Female) %>% summarize('Predictive Parity'=mean(Two_yr_Recidivism=="yes"))%>% select(-Female)

d=compas %>% filter(Two_yr_Recidivism=="yes") %>% group_by(Female) %>% summarize('Balance for the Positive Class'=mean(probability))%>% select(-Female)

cbind(a,b,c,d) %>% knitr::kable() 
```

## But choosing a metric can be complicated 
![](fair_diagram.png)

## {auto-animate="true" visibility="hidden"}
<img src="fair_diagram.png" class="zoom-effect" width="100%">

## Many sources of bias can cause the disparate impact observed by these metrics   {auto-animate="true" .smaller .scrollable}
| **Data Bias**            | **Definition**                                                   | **Main Cause**                                           | **Impact on AI**                                                                 |
|--------------------------|------------------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------------------------------|
| <span style="color:red">**Selection Bias**</span>       | Certain groups are over/under-represented                        | Biased data collection process                           | AI models may not be representative, leading to biased decisions                 |
| <span style="color:red">**Sampling Bias**</span>       | Data are not a random sample                                     | Incomplete or biased sampling                            | Poor generalization to new data, biased predictions                              |
| **Labeling Bias**        | Errors in data labeling                                         | Annotators' biases or societal stereotypes               | AI models learn and perpetuate biased labels                                     |
| **Temporal Bias**        | Historical societal biases                                      | Outdated data reflecting past biases                     | AI models may reinforce outdated biases                                          |
| **Aggregation Bias**     | Data combined from multiple sources                             | Differing biases in individual sources                   | AI models may produce skewed outcomes due to biased data                         |
| <span style="color:red">**Historical Bias**</span>      | Training data reflect past societal biases                      | Biases inherited from historical societal discrimination | Model may perpetuate historical biases and reinforce inequalities                |
| **Measurement Bias**     | Errors or inaccuracies in data collection                       | Data collection process introduces measurement errors    | Model learns from flawed data, leading to inaccurate predictions                 |
| **Confirmation Bias**    | Focus on specific patterns or attributes                        | Data collection or algorithmic bias towards specific features | Model may overlook relevant information and reinforce existing biases       |
| **Proxy Bias**           | Indirect reliance on sensitive attributes                       | Use of correlated proxy variables instead of sensitive attributes | Model indirectly relies on sensitive information, leading to biased outcomes |
| **Cultural Bias**        | Data reflect cultural norms and values                          | Cultural influences in data collection or annotation     | Model predictions may be biased for individuals from different cultural backgrounds |
| **Under-representation Bias** | Certain groups are significantly underrepresented         | Low representation of certain groups in the training data | Model performance is poorer for underrepresented groups                        |
| **Homophily Bias**       | Predictions based on similarity between instances               | Tendency of models to make predictions based on similarity | Model may reinforce existing patterns and exacerbate biases                   |

## How can we mitigate the effect of biases on decision making tools? {auto-animate=true auto-animate-easing="ease-in-out"}
::: {style="text-decoration: underline;"}
Pre-Processing
:::
This is done by modifying your training data before model training

One example is using the Disparate Impact Remover

::: {data-id="Disparate Impact Remover" auto-animate-delay="0.5" style="background: #00000; width: 500px; height: auto; margin: 50px;"}
```{r,include=T,echo=F}
library(fairmodels)

N=1000
data=data.frame(Sex=factor(rbinom(N,1,.5))) %>% mutate(Age=rnorm(N,mean=50+as.numeric(Sex==1)*10,sd=5 + 1*as.numeric(Sex==1)),Time=1)
data %>%
ggplot(aes(x=Age,group=Sex,fill=Sex)) + geom_density(alpha=.25) + 
theme_bw() + ggtitle("Before Disparate Impact Remover") + scale_fill_viridis_d(begin=.25,end=.75)

```
:::
## How can we mitigate the effect of biases on decision making tools? {auto-animate=true auto-animate-easing="ease-in-out" visibility="uncounted"}
::: {style="text-decoration: underline;"}
Pre-Processing
:::
This is done by modifying your training data before model training

One example is using the Disparate Impact Remover 

::: {data-id="Disparate Impact Remover" auto-animate-delay="0.5" style="background: #00000; width: 500px; height: auto; margin: 50px;"}

```{r,include=T,echo=F}
library(gganimate)
data2=rbind(data %>% mutate(lambda=0),disparate_impact_remover(data=data,protected=data$Sex,
features_to_transform="Age",lambda=.5) %>% mutate(Time=2,lambda=.5),
disparate_impact_remover(data=data,protected=data$Sex,
features_to_transform="Age",lambda=.8) %>% mutate(Time=3,lambda=.8),
disparate_impact_remover(data=data,protected=data$Sex,
features_to_transform="Age",lambda=1) %>% mutate(Time=4,lambda=1))

p=data2 %>% ggplot(aes(x=Age,group=Sex,fill=Sex)) + geom_density(alpha=.25) + 
geom_text(aes(x=42,y=.075,label=paste0("lambda: ",round(lambda,2))),size=10) +
theme_bw()+  scale_fill_viridis_d(begin=.25,end=.75) + 
transition_states(Time,transition_length=4, state_length=1)

anim=animate(p,nframes = 100, fps = 10)
#animate(p, nframes = 100, fps = 10, renderer = gifski_renderer("density_animation.gif"))

anim
```
:::


## How can we mitigate the effect of biases on decision making tools? 
::: {style="text-decoration: underline;"}
Pre-Processing
:::
Other examples include methods such as reweighting or re-sampling. 

These methods primarily address bias in the training data but could be used to target certain fairness metrics. 

## How can we mitigate the effect of biases on decision making tools? 
::: {style="text-decoration: underline;"}
In-Processing
:::
- Adversarial Training trains a classifier and an adversary model in parallel
- Classifier is trained to predict the task at hand
- Adversary is trained to exploit a bias. 
- When trained against one another, one can develop a fair model that is simultaneously a strong classifier. 
$$L = L_{\text{task}} - \lambda L_{\text{adv}}$$

## How can we mitigate the effect of biases on decision making tools? {auto-animate=true auto-animate-duration="3" auto-animate-easing="ease-in-out" .smaller}
::: {style="text-decoration: underline;"}
Post-Processing
:::
Threshold Optimization for Equalized Odds
\begin{align*}
P(\widehat{Y}=1|A=a,Y=1) = P(\widehat{Y}=1|A=b,Y=1)
\end{align*}

::: {data-id="ROC" auto-animate-delay="3" data-visibility="visible" style="background: #00000; width: 750px; height: auto; margin: 1px;"}

```{r,include=T,echo=F,cache=TRUE}

library(pROC)
rocs=roc(c2$Two_yr_Recidivism,c2$probability)



  data.frame(
    Sp = rocs$specificities,
    Y = rocs$sensitivities,
    Threshold = rocs$thresholds
  ) %>%
  ggplot(aes(x = 1 - Sp, y = Y)) +
  geom_line() +
  geom_abline(color = "grey") +
  theme_bw() +
  ylab("Sensitivity") +
  xlab("Specificity") +
  theme(aspect.ratio = 0.75) +
  scale_x_continuous(
    breaks = c(0, 0.25, 0.5, 0.75, 1),
    labels = rev(c(0, 0.25, 0.5, 0.75, 1))
  ) +
  geom_vline(
    aes(xintercept = 1-.699),
    linetype = "dashed"
  ) +
  geom_hline(
    aes(yintercept = .634),
    linetype = "dashed"
  ) +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + ggtitle("Threshold = 0.50")

```
:::

## How can we mitigate the effect of biases on decision making tools? {auto-animate=true auto-animate-duration="3" auto-animate-easing="ease-in-out" visibility="uncounted" .smaller}
::: {style="text-decoration: underline;"}
Post-Processing
:::
Threshold Optimization for Equalized Odds
\begin{align*}
P(\widehat{Y}=1|A=a,Y=1) = P(\widehat{Y}=1|A=b,Y=1)
\end{align*}

::: {data-id="ROC" auto-animate-delay="3" data-visibility="visible" style="background: #00000; width: 750px; height: auto; margin: 1px;"}

```{r,include=T,echo=F,cache=TRUE}

library(pROC)
rocs=c2 %>% split(.$Female) %>% purrr::map(function(x) roc(x$Two_yr_Recidivism,x$probability))  

vline_data <- data.frame(
  xintercept = c(1 - 0.679, 1 - 0.3753),
  Sex = c("Male", "Female")
)

# Data frame for horizontal dashed lines
hline_data <- data.frame(
  yintercept = c(0.652, 0.85),
  Sex = c("Male", "Female")
)

map2_df(rocs, c("Male", "Female"), function(x, y) {
  data.frame(
    Sp = x$specificities,
    Y = x$sensitivities,
    Sex = y,
    Threshold = x$thresholds
  )
}) %>%
  ggplot(aes(x = 1 - Sp, y = Y, color = Sex, group = Sex)) +
  geom_line() +
  geom_abline(color = "grey") +
  theme_bw() +
  ylab("Sensitivity") +
  xlab("Specificity") +
  theme(aspect.ratio = 0.75) +
  scale_x_continuous(
    breaks = c(0, 0.25, 0.5, 0.75, 1),
    labels = rev(c(0, 0.25, 0.5, 0.75, 1))
  ) +
  geom_vline(
    data = vline_data,
    aes(xintercept = xintercept, color = Sex),
    linetype = "dashed"
  ) +
  geom_hline(
    data = hline_data,
    aes(yintercept = yintercept, color = Sex),
    linetype = "dashed"
  ) +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + ggtitle("Threshold = 0.50")

```
:::

## How can we mitigate the effect of biases on decision making tools? {auto-animate=true  auto-animate-easing="ease-in-out" visibility="uncounted" .smaller}
::: {style="text-decoration: underline;"}
Post-Processing
:::
Threshold Optimization for Equalized Odds
\begin{align*}
P(\widehat{Y}=1|A=a,Y=1) = P(\widehat{Y}=1|A=b,Y=1)
\end{align*}

::: {data-id="ROC" style="background: #00000; width: 750px; height: auto; margin: 1px;"}
```{r,include=T,echo=F,cache=TRUE}
library(pROC)
rocs=c2 %>% split(.$Female) %>% purrr::map(function(x) roc(x$Two_yr_Recidivism,x$probability))  

vline_data <- data.frame(
  xintercept =  c(1 - 0.679, 1 - 0.3753,1 - 0.391, 1 - 0.3753),
  Sex = c("Male", "Female","Male","Female"),
  Time = c(1,1,2,2)
)

# Data frame for horizontal dashed lines
hline_data <- data.frame(
  yintercept = c(0.652, 0.85,0.851, 0.85),
  Sex = c("Male", "Female","Male","Female"),
  Time = c(1,1,2,2)
)
p=map2_df(rocs, c("Male", "Female"), function(x, y) {
  data.frame(
    Sp = x$specificities,
    Y = x$sensitivities,
    Sex = y,
    Threshold = x$thresholds
  )
}) %>%
  ggplot(aes(x = 1 - Sp, y = Y, color = Sex, group = Sex)) +
  geom_line() +
  geom_abline(color = "grey") +
  theme_bw() +
  ylab("Sensitivity") +
  xlab("Specificity") +
  theme(aspect.ratio = 0.75) +
  scale_x_continuous(
    breaks = c(0, 0.25, 0.5, 0.75, 1),
    labels = rev(c(0, 0.25, 0.5, 0.75, 1))
  ) +
  geom_vline(
    data = vline_data,
    aes(xintercept = xintercept, color = Sex),
    linetype = "dashed"
  ) +
  geom_hline(
    data = hline_data,
    aes(yintercept = yintercept, color = Sex),
    linetype = "dashed"
  ) +
  scale_color_viridis_d(begin = 0.25, end = 0.75) + ggtitle("Female Threshold = 0.50, Male Threshold = 0.40") +
  transition_states(Time,transition_length=2, state_length=1)

anim=animate(p,nframes = 100, fps = 10)
#animate(p, nframes = 100, fps = 10, renderer = gifski_renderer("density_animation.gif"))

anim

```
:::

## How can we mitigate the effect of biases on decision making tools? {auto-animate=true  auto-animate-easing="ease-in-out"}
::: {style="text-decoration: underline;"}
Post-Processing
:::
And other approaches: 

- Calibration Post-Processing
- Reject Option Classification (abstain in high fairness concern cases)
- Equalized Odds Post-Processing (Adjust model predictions to ensure EO)

## How can we mitigate the effect of biases on decision making tools? 
::: {style="text-decoration: underline;"}
Regularization-Based
:::
::: {.fragment width="450" height="300" index="1"}
- Tries to minimize the negative log likelihood of the model 
- But also includes a penalty enforcing a concept of fairness 
:::
::: {.fragment width="450" height="300" index="2"}

E.g. take a logistic regression model

```{r, include=T,echo=T}
log_likelihood <- function(beta, X, Y) {
  logit <- as.matrix(X) %*% beta
  p <- plogis(logit)
  logLL=-(sum(Y * log(p) + (1 - Y) * log(1 - p))) # Negative Log-likehood
  logLL
}
```
:::

## How can we mitigate the effect of biases on decision making tools? {visibility="uncounted"}
::: {style="text-decoration: underline;"}
Regularization-Based
:::
- Tries to minimize the negative log likelihood of the model 
- But also includes a penalty enforcing a concept of fairness 

E.g. take a logistic regression model and add a penalty term 

```{r, include=T,echo=T}
log_likelihood <- function(beta, X, Y,A,lam1=1) {
  logit <- as.matrix(X) %*% beta
  p <- plogis(logit)
  pA1=p[which(A=="F" & Y==1)] # probability of being positive given A="F"
  pA0=p[which(A=="M" & Y==1)] # probability of being positive given A="M"
  pen1=abs(mean(pA1)-mean(pA0)) # How different are the probabilities on average? 
  logLL=-(sum(Y * log(p) + (1 - Y) * log(1 - p))) + lam1*log(pen1) # Add the penalty term
  logLL
}
```

## {auto-animate="true"}
::: {.fragment .fade-up .highlight-current-red}
Questions?
:::