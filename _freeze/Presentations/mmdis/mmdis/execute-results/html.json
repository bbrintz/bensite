{
  "hash": "13a2102fd5a1dbad4d9ef578ea8b94d2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Measuring and Mitigating Disparity of Decision-Making Tools\"\nauthor: \"Ben Brintz\"\ninstitute: \"Division of Epidemiology\"\nformat: \n  revealjs:\n    auto-animate-easing: ease-in-out\n    auto-animate-unmatched: false\n    auto-animate-duration: 2\n    revealjs-plugins: \n    theme: white\n    incremental: true\n    slideNumber: false  # Slide numbers (optional)\n    transition: fade   # Smooth slide transitions\n    title-slide-background: \"white\" # Ensure title slide background matches your style\n    css: custom.css    # Link to your custom CSS file\n---\n\n\n<div style=\"font-size: 50px; text-align: center;\">\nWhat do I mean by decision-making tools? \n</div>\n- Any system, algorithm, model, or process that automates or supplements decisions \n- Clinical prediction, finance, employment, and law enforcement\n- Output is most commonly a risk probability (0,1) or a score (Decision?) \n\n::: {.callout-caution title=\"Caution\" .fragment}\nDisadvantaged/Sensitive groups are often included as features in models but ignored when assessing the performance of these tools\n:::\n\n## There is some controversy surrounding the eGFR equation {style=\"font-size:.75em;\" auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n## There is some controversy surrounding the eGFR equation {style=\"font-size:.75em;\" auto-animate=true auto-animate-easing=\"ease-in-out\" visibility=\"uncounted\"}\n::: {.r-hstack}\n  <img src=\"egfr1.gif\" alt=\"GIF 1\" style=\"height:160px; width:auto; object-fit:contain;\">\n  <img src=\"egfr2.gif\" alt=\"GIF 2\" style=\"height:160px; width:auto; object-fit:contain;\">\n  <img src=\"egfr3.gif\" alt=\"GIF 3\" style=\"height:160px; width:auto; object-fit:contain;\">\n:::\n\n## There is some controversy surrounding the eGFR equation {style=\"font-size:.75em;\" auto-animate=true auto-animate-easing=\"ease-in-out\" visibility=\"uncounted\"}\n::: {.r-hstack}\n  <img src=\"egfr1.gif\" alt=\"GIF 1\" style=\"height:160px; width:auto; object-fit:contain;\">\n  <img src=\"egfr2.gif\" alt=\"GIF 2\" style=\"height:160px; width:auto; object-fit:contain;\">\n  <img src=\"egfr3.gif\" alt=\"GIF 3\" style=\"height:160px; width:auto; object-fit:contain;\">\n:::\n::: {.r-hstack}\n  <img src=\"egfr4.gif\" alt=\"GIF 1\" style=\"height:160px; width:auto; object-fit:contain;\">\n  <img src=\"egfr5.gif\" alt=\"GIF 2\" style=\"height:160px; width:auto; object-fit:contain;\">\n  <img src=\"egfr6.gif\" alt=\"GIF 3\" style=\"height:160px; width:auto; object-fit:contain;\">\n:::\n\n## There is some controversy surrounding the eGFR equation {style=\"font-size:.75em;\" auto-animate=true auto-animate-easing=\"ease-in-out\" visibility=\"uncounted\"}\n::: {.r-hstack}\n  <img src=\"egfr1.gif\" alt=\"GIF 1\" style=\"height:160px; width:auto; object-fit:contain;\">\n  <img src=\"egfr2.gif\" alt=\"GIF 2\" style=\"height:160px; width:auto; object-fit:contain;\">\n  <img src=\"egfr3.gif\" alt=\"GIF 3\" style=\"height:160px; width:auto; object-fit:contain;\">\n:::\n::: {.r-hstack}\n  <img src=\"egfr4.gif\" alt=\"GIF 1\" style=\"height:160px; width:auto; object-fit:contain;\">\n  <img src=\"egfr5.gif\" alt=\"GIF 2\" style=\"height:160px; width:auto; object-fit:contain;\">\n  <img src=\"egfr6.gif\" alt=\"GIF 3\" style=\"height:160px; width:auto; object-fit:contain;\">\n:::\n::: {.r-hstack}\n  <img src=\"egfr7.gif\" alt=\"GIF 1\" style=\"height:160px; width:auto; object-fit:contain;\">\n  <img src=\"egfr8.gif\" alt=\"GIF 2\" style=\"height:160px; width:auto; object-fit:contain;\">\n  <img src=\"egfr9.gif\" alt=\"GIF 3\" style=\"height:160px; width:auto; object-fit:contain;\">\n:::\n\n---\n\n<div style=\"font-size: 50px; text-align: center;\"> The NKF and ASN have since recommended removal of race from the equation</div>\n\n\n- Acknowledged race is a social concept \n    - i.e., it's a system to classify individuals rather than reflect biology\n-  Does removal of race reduce performance of the decision-making tool?\n\n\n\n::: {.fragment .fade-up}\n It depends on how you're measuring performance\n:::\n\n---\n\n<div style=\"font-size: 60px; text-align: center;\">Performance metrics are a trade-off </div>\n\n::: {.cell}\n::: {.cell-output-display}\n![](mmdis_files/figure-revealjs/unnamed-chunk-1-1.png){width=960}\n:::\n:::\n\n::: {.fragment .fade-in-then-semi-out}\nThe developer of a tool can choose to emphasize one metric over another \n:::\n\n::: {.fragment .fade-right}\nAnd the choice of metric could be predictive performance or fairness or you could consider both\n:::\n\n\n## Some fairness metrics are more well known than others {.smaller .scrollable}\n<style>\n.reveal .fragment {\n  margin-bottom: 5px; /* Adjust this value to reduce space between fragments */\n}\n</style>\n\n\n::: {.fragment  fragment-index=1}\n\\begin{align*}\n\\text{Statistical Parity} &= P(\\widehat{Y}=1|A=a) \\\\\n&= P(\\widehat{Y}=1|A=b)\n\\end{align*}\n:::\n\n::: {.fragment fragment-index=2}\n\\begin{align*}\n\\text{Equalized Odds} &= P(\\widehat{Y}=1|A=a,Y=1) \\\\\n&= P(\\widehat{Y}=1|A=b,Y=1)\n\\end{align*}\n:::\n\n::: {.fragment fragment-index=3}\n\\begin{align*}\n\\text{Predictive Parity} &= P(Y=1|\\widehat{Y}=1,A=a) \\\\\n&= P(Y=1|\\widehat{Y}=1,A=b)\n\\end{align*}\n:::\n\n::: {.fragment fragment-index=4}\n\\begin{align*}\n\\text{Balance for the Positive Class} &= E(S|Y=1,A=a) \\\\ &=E(S|Y=1,A=b)\n\\end{align*}\n:::\n\n\n## I'm going to apply these metrics to the COMPAS data {auto-animate=\"true\" .scrollable}\n\n- A landmark dataset to study algorithmic fairness in recidivism prediction\n- You can access this data in R through the fairness package\n\n::: {.fragment .fade-up index=2}\n```r\nlibrary(fairness)\n\nhead(compas)\n```\n:::\n\n## I'm going to apply these metrics to the COMPAS data {auto-animate=\"true\" visibility=\"uncounted\" .scrollable}\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   Two_yr_Recidivism Number_of_Priors Age_Above_FourtyFive Age_Below_TwentyFive\n4                 no       -0.6843578                   no                   no\n5                yes        2.2668817                   no                   no\n7                 no       -0.6843578                   no                   no\n11                no       -0.6843578                   no                   no\n14                no       -0.6843578                   no                   no\n24                no       -0.6843578                   no                   no\n   Female Misdemeanor        ethnicity probability predicted\n4    Male         yes            Other   0.3151557         0\n5    Male          no        Caucasian   0.8854616         1\n7  Female         yes        Caucasian   0.2552680         0\n11   Male          no African_American   0.4173908         0\n14   Male         yes         Hispanic   0.3200982         0\n24   Male         yes            Other   0.3151557         0\n```\n\n\n:::\n:::\n\n\n\n\n## Measuring fairness can take just a few lines of code {auto-animate=\"true\" .smaller .scrollable }\n```r\na=compas %>% group_by(Female) %>% summarize(`Statistical Parity`=mean(predicted))\n\nb=compas %>% filter(Two_yr_Recidivism==\"yes\") %>% group_by(Female) %>% summarize(`Equalized Odds`=mean(predicted))\n\nc=compas %>% filter(predicted==1) %>% group_by(Female) %>% summarize('Predictive Parity'=mean(Two_yr_Recidivism==\"yes\"))\n\nd=compas %>% filter(Two_yr_Recidivism==\"yes\") %>% group_by(Female) %>% summarize('Balance for the Positive Class'=mean(probability))\n```\n\n## Measuring fairness can take just a few lines of code {auto-animate=\"true\" .smaller .scrollable visibility=\"uncounted\"}\n\n::: {.cell}\n\n```{.r .cell-code}\na=compas %>% group_by(Sex=Female) %>% summarize(`Statistical Parity`=mean(predicted))\n\nb=compas %>% filter(Two_yr_Recidivism==\"yes\") %>% group_by(Female) %>% summarize(`Equalized Odds`=mean(predicted)) %>% select(-Female)\n\nc=compas %>% filter(predicted==1) %>% group_by(Female) %>% summarize('Predictive Parity'=mean(Two_yr_Recidivism==\"yes\"))%>% select(-Female)\n\nd=compas %>% filter(Two_yr_Recidivism==\"yes\") %>% group_by(Female) %>% summarize('Balance for the Positive Class'=mean(probability))%>% select(-Female)\n\ncbind(a,b,c,d) %>% knitr::kable() \n```\n\n::: {.cell-output-display}\n\n\n|Sex    | Statistical Parity| Equalized Odds| Predictive Parity| Balance for the Positive Class|\n|:------|------------------:|--------------:|-----------------:|------------------------------:|\n|Male   |          0.5069041|      0.6794658|         0.6427161|                      0.5902647|\n|Female |          0.2221277|      0.3753027|         0.5938697|                      0.4567142|\n\n\n:::\n:::\n\n---\n\n<div style=\"font-size: 45px; text-align: center; margin-bottom: 1px\">  But choosing a metric can be complicated </div>\n<img src=\"fair_diagram.png\" style=\"width: 100%; height: auto;\">\n\n## {auto-animate=\"true\" visibility=\"hidden\"}\n<img src=\"fair_diagram.png\" class=\"zoom-effect\" width=\"100%\">\n\n## Many sources of bias can cause the disparate impact observed by these metrics   {auto-animate=\"true\" .smaller .scrollable}\n| **Data Bias**            | **Definition**                                                   | **Main Cause**                                           | **Impact on AI**                                                                 |\n|--------------------------|------------------------------------------------------------------|----------------------------------------------------------|----------------------------------------------------------------------------------|\n| **Selection Bias**       | Certain groups are over/under-represented                        | Biased data collection process                           | AI models may not be representative, leading to biased decisions                 |\n| <span style=\"color:red\">**Sampling Bias**</span>       | Data are not a random sample                                     | Incomplete or biased sampling                            | Poor generalization to new data, biased predictions                              |\n| **Labeling Bias**        | Errors in data labeling                                         | Annotators' biases or societal stereotypes               | AI models learn and perpetuate biased labels                                     |\n| **Temporal Bias**        | Historical societal biases                                      | Outdated data reflecting past biases                     | AI models may reinforce outdated biases                                          |\n| **Aggregation Bias**     | Data combined from multiple sources                             | Differing biases in individual sources                   | AI models may produce skewed outcomes due to biased data                         |\n| <span style=\"color:red\">**Historical Bias**</span>      | Training data reflect past societal biases                      | Biases inherited from historical societal discrimination | Model may perpetuate historical biases and reinforce inequalities                |\n| **Measurement Bias**     | Errors or inaccuracies in data collection                       | Data collection process introduces measurement errors    | Model learns from flawed data, leading to inaccurate predictions                 |\n| **Confirmation Bias**    | Focus on specific patterns or attributes                        | Data collection or algorithmic bias towards specific features | Model may overlook relevant information and reinforce existing biases       |\n| <span style=\"color:red\">**Proxy Bias** </span>          | Indirect reliance on sensitive attributes                       | Use of correlated proxy variables instead of sensitive attributes | Model indirectly relies on sensitive information, leading to biased outcomes |\n| **Cultural Bias**        | Data reflect cultural norms and values                          | Cultural influences in data collection or annotation     | Model predictions may be biased for individuals from different cultural backgrounds |\n| **Under-representation Bias** | Certain groups are significantly underrepresented         | Low representation of certain groups in the training data | Model performance is poorer for underrepresented groups                        |\n| **Homophily Bias**       | Predictions based on similarity between instances               | Tendency of models to make predictions based on similarity | Model may reinforce existing patterns and exacerbate biases                   |\n\n## How can we mitigate the effect of biases on decision making tools? {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n- Pre-Processing - modifying your training data\n- In-Processing - modifying the training process\n- Post-Processing - modifying the output of the model\n- Regularization-Based - modifying the model itself\n\n## How can we mitigate the effect of biases on decision making tools? {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n::: {style=\"text-decoration: underline;\"}\nPre-Processing\n:::\nThis is done by modifying your training data before model training\n\nOne example is using the Disparate Impact Remover\n\n::: {data-id=\"Disparate Impact Remover\" auto-animate-delay=\"0.5\" style=\"background: #00000; width: 500px; height: auto; margin: 50px;\"}\n::: {.cell}\n::: {.cell-output-display}\n![](mmdis_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n:::\n## How can we mitigate the effect of biases on decision making tools? {auto-animate=true auto-animate-easing=\"ease-in-out\" visibility=\"uncounted\"}\n::: {style=\"text-decoration: underline;\"}\nPre-Processing\n:::\nThis is done by modifying your training data before model training\n\nOne example is using the Disparate Impact Remover \n\n::: {data-id=\"Disparate Impact Remover\" auto-animate-delay=\"0.5\" style=\"background: #00000; width: 500px; height: auto; margin: 50px;\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](mmdis_files/figure-revealjs/unnamed-chunk-6-1.gif)\n:::\n:::\n:::\n\n\n## How can we mitigate the effect of biases on decision making tools? \n::: {style=\"text-decoration: underline;\"}\nPre-Processing\n:::\nOther examples include methods such as reweighting or re-sampling. \n\nThese methods primarily address bias in the training data but could be used to target certain fairness metrics. \n\n## How can we mitigate the effect of biases on decision making tools? \n::: {style=\"text-decoration: underline;\"}\nIn-Processing\n:::\n- Adversarial Training trains a classifier and an adversary model in parallel\n- Classifier is trained to predict the task at hand\n- Adversary is trained to exploit a bias. \n- When trained against one another, one can develop a fair model that is simultaneously a strong classifier. \n$$L = L_{\\text{task}} - \\lambda L_{\\text{adv}}$$\n\n## How can we mitigate the effect of biases on decision making tools? {auto-animate=true auto-animate-duration=\"3\" auto-animate-easing=\"ease-in-out\" .smaller}\n::: {style=\"text-decoration: underline;\"}\nPost-Processing\n:::\nThreshold Optimization for Equalized Odds (COMPAS)\n\\begin{align*}\nP(\\widehat{Y}=1|A=a,Y=1) = P(\\widehat{Y}=1|A=b,Y=1)\n\\end{align*}\n\n::: {data-id=\"ROC\" auto-animate-delay=\"3\" data-visibility=\"visible\" style=\"background: #00000; width: 750px; height: auto; margin: 1px;\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](mmdis_files/figure-revealjs/unnamed-chunk-7-1.png){width=960}\n:::\n:::\n:::\n\n## How can we mitigate the effect of biases on decision making tools? {auto-animate=true auto-animate-duration=\"3\" auto-animate-easing=\"ease-in-out\" visibility=\"uncounted\" .smaller}\n::: {style=\"text-decoration: underline;\"}\nPost-Processing\n:::\nThreshold Optimization for Equalized Odds (COMPAS)\n\\begin{align*}\nP(\\widehat{Y}=1|A=a,Y=1) = P(\\widehat{Y}=1|A=b,Y=1)\n\\end{align*}\n\n::: {data-id=\"ROC\" auto-animate-delay=\"3\" data-visibility=\"visible\" style=\"background: #00000; width: 750px; height: auto; margin: 1px;\"}\n\n::: {.cell}\n::: {.cell-output-display}\n![](mmdis_files/figure-revealjs/unnamed-chunk-8-1.png){width=960}\n:::\n:::\n:::\n\n## How can we mitigate the effect of biases on decision making tools? {auto-animate=true  auto-animate-easing=\"ease-in-out\" visibility=\"uncounted\" .smaller}\n::: {style=\"text-decoration: underline;\"}\nPost-Processing\n:::\nThreshold Optimization for Equalized Odds (COMPAS)\n\\begin{align*}\nP(\\widehat{Y}=1|A=a,Y=1) = P(\\widehat{Y}=1|A=b,Y=1)\n\\end{align*}\n\n::: {data-id=\"ROC\" style=\"background: #00000; width: 750px; height: auto; margin: 1px;\"}\n::: {.cell}\n::: {.cell-output-display}\n![](mmdis_files/figure-revealjs/unnamed-chunk-9-1.gif)\n:::\n:::\n:::\n\n## How can we mitigate the effect of biases on decision making tools? {auto-animate=true  auto-animate-easing=\"ease-in-out\"}\n::: {style=\"text-decoration: underline;\"}\nPost-Processing\n:::\nAnd other approaches: \n\n- Calibration Post-Processing\n- Reject Option Classification (abstain in high fairness concern cases)\n- Equalized Odds Post-Processing (Adjust model predictions to ensure EO)\n\n## How can we mitigate the effect of biases on decision making tools? \n::: {style=\"text-decoration: underline;\"}\nRegularization-Based\n:::\n::: {.fragment width=\"450\" height=\"300\" index=\"1\"}\n- Tries to minimize the negative log likelihood of the model \n- But also includes a penalty enforcing a concept of fairness \n:::\n::: {.fragment width=\"450\" height=\"300\" index=\"2\"}\n\nE.g. take a logistic regression model\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_likelihood <- function(beta, X, Y) {\n  logit <- as.matrix(X) %*% beta\n  p <- plogis(logit)\n  logLL=-(sum(Y * log(p) + (1 - Y) * log(1 - p))) # Negative Log-likehood\n  logLL\n}\n```\n:::\n:::\n\n## How can we mitigate the effect of biases on decision making tools? {visibility=\"uncounted\"}\n::: {style=\"text-decoration: underline;\"}\nRegularization-Based\n:::\n- Tries to minimize the negative log likelihood of the model \n- But also includes a penalty enforcing a concept of fairness (BoPC)\n\nE.g. take a logistic regression model and add a penalty term \n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|4,5,6,8\"}\nlog_likelihood <- function(beta, X, Y,A,lam1=1) {\n  logit <- as.matrix(X) %*% beta\n  p <- plogis(logit)\n  pA1=p[which(A==\"F\" & Y==1)] # probability of being positive given A=\"F\"\n  pA0=p[which(A==\"M\" & Y==1)] # probability of being positive given A=\"M\"\n  pen1=abs(mean(pA1)-mean(pA0)) # How different are the probabilities on average? \n  logLL=-(sum(Y * log(p) + (1 - Y) * log(1 - p))) # Add the penalty term\n  logLL + lam1*log(pen1) \n}\n```\n:::\n\n## Final Thoughts  \n- Cross-validation is a great tool to assess the performance/fairness of a model and tune hyperparameters\n- But prospective External Validation is still necessary \n- It is important to consider the effect on subgroups and consider the trade-offs between fairness and predictive performance in certain tools\n\n\n## {auto-animate=\"true\"}\n::: {.fragment .fade-up .highlight-current-red}\nQuestions?\n\n\n:::\n\n## References\n\n1. Chen P, Wu L, Wang L. AI fairness in data management and analytics: A review on challenges, methodologies and applications. Applied Sciences. 2023 Sep 13;13(18):10258.\n\n2. Makhlouf K, Zhioua S, Palamidessi C. Machine learning fairness notions: Bridging the gap with real-world applications. Information Processing & Management. 2021 Sep 1;58(5):102642.\n\n3. Yang J, Soltan AA, Eyre DW, Yang Y, Clifton DA. An adversarial training framework for mitigating algorithmic biases in clinical machine learning. NPJ digital medicine. 2023 Mar 29;6(1):55.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}